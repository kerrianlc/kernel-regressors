{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La vraisemblance de $n$ variables aléatoires de Bernoulli peut être exprimée ainsi: $\\mathcal{L}(\\beta) = \\prod\\limits_{i=1}^n \\pi(\\bm{x}_i)^{y_i}(1-\\pi(\\bm{x}_i))^{1-y_i}$. On obtient alors la log-vraisemblance $\\mathcal{l}(\\beta) = \\log(\\mathcal{L}(\\beta))$, et connaissant l'expression de $\\pi(\\bm{x}_i)$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathcal{l}(\\beta) = \\sum\\limits_{i=1}^n y_i\\beta^T \\bm{x}_i - \\log(1+e^{\\beta^T \\bm{x}_i})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\nabla_\\beta \\mathcal{l}(\\beta) = \\sum\\limits_{i=1}^n y_i\\bm{x}_i - \\frac{x_ie^{\\beta^T \\bm{x}_i}}{1 + x_ie^{\\beta^T \\bm{x}_i}} = \\sum\\limits_{i=1}^n (y_i-\\pi(\\bm{x}_i))x_i$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En notation matricielle:\n",
    "\n",
    "$$\\nabla_\\beta\\mathcal{l}(\\beta) = X^T(\\bm{y} - \\bm{\\pi})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\nabla_\\beta (\\nabla_\\beta l(\\beta)) = \\nabla_\\beta \\sum\\limits_{i=1}^n [y_i - \\pi(x_i)]x_i = \\sum\\limits_{i=1}^n \\nabla_\\beta [y_i - \\pi(x_i)]x_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soit,\n",
    "$$\\nabla_\\beta (\\nabla_\\beta l(\\beta)) = \\sum\\limits_{i=1}^n \\nabla_\\beta \\left[\\frac{1}{1 + e^{-\\beta^Tx_i}} \\right] x_i = \\sum\\limits_{i=1}^n \\left[\\frac{1}{1 + e^{-\\beta^Tx_i}} \\right]^2 e^{-\\beta^Tx_i} (-x_i)^T x_i $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D'où:\n",
    "\n",
    "$$\\nabla_\\beta (\\nabla_\\beta l(\\beta)) = -\\sum\\limits_{i=1}^n \\left[\\frac{e^{-\\beta^Tx_i}}{1 + e^{-\\beta^Tx_i}} \\right] \\left[\\frac{1}{1 + e^{-\\beta^Tx_i}} \\right] x_i^T x_i = -\\sum\\limits_{i=1}^n \\pi(x_i)[1 - \\pi(x_i)] x_i^T x_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puis en notation matricielle $\\nabla_\\beta (\\nabla_\\beta l(\\beta)) = -X^TVX$ avec $V=\\text{diag}(\\pi(x_i)[1 - \\pi(x_i)]_{1\\leq i\\leq n})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le développement en série de Taylor de $\\mathcal{l}(\\beta)$ est $\\begin{equation}\\mathcal{l}(\\beta) = \\mathcal{l}(\\beta^{(s)}) + \\nabla_\\beta\\mathcal{l}(\\beta^{(s)})^T(\\beta - \\beta^{(s)}) + \\frac{1}{2}(\\beta - \\beta^{(s)})^T\\nabla\\nabla_\\beta\\mathcal{l}(\\beta^{(s)})(\\beta - \\beta^{(s)}) + r(\\beta - \\beta^{(s)}),\\end{equation}$ où $r(\\beta - \\beta^{(s)})$  est petit comparé à $\\|\\beta - \\beta^{(s)}\\|$. En dérivant $\\mathcal{l}(\\beta)$ selon $\\beta$, pris au maximum, on trouve l'équation:\n",
    "$\\begin{equation}0 =  \\nabla_\\beta\\mathcal{l}(\\beta^{(s)}) + \\nabla\\nabla_\\beta\\mathcal{l}(\\beta^{(s)})(\\beta - \\beta^{(s)}).\\end{equation}$ Et finalement, on trouve la solution $\\beta^{(s+1)}$ telle que:\n",
    "$\\begin{equation}\\beta^{(s+1)} =  \\beta^{(s)} - (\\nabla\\nabla_\\beta\\mathcal{l}(\\beta^{(s)}))^{-1}\\nabla_\\beta\\mathcal{l}(\\beta^{(s)}),\\end{equation}$ donc on trouve la formule itérative à $s$ : $\\beta^{(s+1)} =  \\beta^{(s)} + (X^TV^{(s)}X)^{-1}X^T(\\bm{y} - \\bm{\\pi}).$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme le polynôme $X\\mapsto X(1-X)$ est majoré par $\\frac{1}{4}$ pour $X=\\frac{1}{2}$, on trouve que $\\pi^{(s)}(x_i)[1 - \\pi^{(s)}(x_i)]\\leq \\frac{1}{4}$, en prenant $H_2 = -(\\frac{1}{4} + \\varepsilon)X^TX$, avec $\\varepsilon > 0$ on trouve que pour tout $v$: $v^T(H_1 -H_2)v=(Xv)^T(\\frac{1}{2}I_n - V^{(s)})Xv=u^T((\\frac{1}{4} + \\varepsilon)I_n - V^{(s)})u>0$ avec $u=Xv$ et donc $H_1 -H_2$ est définie positive.\n",
    "\n",
    "\n",
    "**Question 4.**\n",
    "\n",
    "\n",
    "En prenant la limite lorsque $\\varepsilon\\rightarrow 0$, $\\beta^{(s+1)} =  \\beta^{(s)} + 4(X^TX)^{-1}X^T(\\bm{y} - \\bm{\\pi}).$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5.**\n",
    "\n",
    "- Le terme de pénalité assure plus de stabilité dans l'estimateur $\\hat\\beta$, on s'assure qu'il reste dans une boule d'un rayon fixé $R$, $\\|\\beta\\|_2^2<R^2$.\n",
    "- En grande dimension, pour peu d'observations, le terme rend la matrice définie positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les expressions du gradient et de la hessienne deviennent alors:\n",
    "\n",
    "$$\\nabla_\\beta\\mathcal{l}(\\beta) = X^T(\\bm{y} - \\bm{\\pi}) - \\lambda\\beta$$\n",
    "$$\\nabla_\\beta (\\nabla_\\beta l(\\beta)) = -X^TVX -\\lambda I_p$$\n",
    "\n",
    "Donc finalement: $\\beta_\\lambda^{(s+1)} =  \\beta_\\lambda^{(s)} + 4(X^TX + 4\\lambda I_p)^{-1}(X^T(\\bm{y} - \\bm{\\pi})- \\lambda\\beta_\\lambda^{(s)}).$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathcal{l}(\\beta) = \\sum\\limits_{i=1}^n y_i\\beta^T \\bm{x}_i - \\log(1+e^{\\beta^T \\bm{x}_i})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 7.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximiser la log-vraisemblence pénalisée équivaut à minimiser la quantité selon $\\beta\\in\\mathbb{R}^p$:\n",
    "\n",
    "$\\begin{equation}\\sum\\limits_{i=1}^n \\log(1+e^{\\beta^T \\bm{x}_i}) + \\frac{\\lambda}{2}\\|\\beta\\|_2^2 -y_i\\beta^T \\bm{x}_i=\\sum\\limits_{i=1}^n \\log(e^{-y_i\\beta^Tx_i}(1+e^{\\beta^T \\bm{x}_i})) + \\frac{\\lambda}{2}\\|\\beta\\|_2^2=\\sum\\limits_{i=1}^n \\log(e^{-y_i\\beta^Tx_i}+e^{(1-y_i)\\beta^T \\bm{x}_i}) + \\frac{\\lambda}{2}\\|\\beta\\|_2^2.\\end{equation}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On définit la variable $\\overline y_i = 2 y_i - 1\\in\\{-1,1\\}$ et on remarque que:\n",
    "\n",
    "$$\\begin{cases}\n",
    "\\log(e^{-y_i\\beta^Tx_i}+e^{(1-y_i)\\beta^T \\bm{x}_i}) = \\log(1+e^{\\beta^T \\bm{x}_i}) & y_i = 0 & \\overline y_i = -1\\\\\n",
    "\\log(e^{-y_i\\beta^Tx_i}+e^{(1-y_i)\\beta^T \\bm{x}_i}) = \\log(1+e^{-\\beta^T \\bm{x}_i}) & y_i = 1 & \\overline y_i = 1\\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "On peut donc réécrire l'expression dans tous les cas sous la forme $\\log(1+e^{-\\overline y_i\\beta^T \\bm{x}_i})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le problème d'optimisation est alors $\\min\\limits_{\\beta\\in\\mathbb{R}^p}\\sum\\limits_{i=1}^n \\log(1+e^{-\\overline y_i\\beta^T \\bm{x}_i}) + \\frac{\\lambda}{2}\\|\\beta\\|_2^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**Question 8.**\n",
    "\n",
    "On pose $l_i(z) = \\log(1+\\exp(-\\overline y_iz))$. On reformule ainsi le problème d'optiminsation avec les contraintes $\\forall i\\ \\beta^Tx_i = z_i$. Le laplacien devient ainsi $\\min\\limits_{\\beta\\in\\mathbb{R}^p}\\sum\\limits_{i=1}^n l_i(z_i) + \\frac{\\lambda}{2}\\|\\beta\\|_2^2 + \\sum\\limits_{i=1}^n\\alpha_i(\\beta^Tx_i - z_i)$.\n",
    "\n",
    "\n",
    "On calcule le dual de Fenchel-Rockafeller, or $\\min\\limits_{\\beta\\in\\mathbb{R}^p}\\frac{\\lambda}{2}\\|\\beta\\|_2^2+ \\alpha^TX\\beta=-\\frac{1}{2\\lambda}\\alpha^T X X^T\\alpha$ et pour tout $i$, on trouve $l_i^*(\\alpha_i)=\\frac{-\\alpha_i}{\\overline y_i}\\log(-\\frac{\\alpha_i}{\\overline y_i}) + \\left(1+\\frac{\\alpha_i}{\\overline y_i}\\right)\\log\\left(1+\\frac{\\alpha_i}{\\overline y_i}\\right)$, en notant que $-\\frac{\\alpha_i}{\\overline y_i} = \\sigma(\\overline y_iz)$ et $\\text{logit} \\circ \\sigma = \\text{id}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "En procédant au changement de variable $\\gamma_i = -\\frac{\\alpha_i}{\\overline y_i}$, le problème dual est donc $\\max\\limits_{\\gamma\\in\\mathbb{R}^n}\\sum\\limits_{i=1}^n -\\gamma_i\\log(\\gamma_i) - \\left(1-\\gamma_i\\right)\\log\\left(1-\\gamma_i\\right) - \\frac{1}{2\\lambda}\\gamma^T\\text{diag}(\\overline y)X X^T\\text{diag}(\\overline y)\\gamma$. On notera $K:=\\text{diag}(\\overline y)X X^T\\text{diag}(\\overline y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En considérant maintenant le gradient du laplacien précédent: $\\nabla_{\\gamma} \\mathcal{D}(\\gamma)=-\\text{logit}(\\gamma) - \\frac{1}{\\lambda}K\\gamma$.\n",
    "\n",
    "\n",
    "$\\nabla\\nabla_\\gamma \\mathcal{D}(\\gamma) = - \\text{diag}\\left(\\frac{1}{\\gamma(1-\\gamma)}\\right) - \\frac{1}{\\lambda}K\\simeq - 4I_n - \\frac{1}{\\lambda}K.$\n",
    "\n",
    "\n",
    "Donc, $\\gamma^{(s+1)} = \\gamma^{(s)} + \\text{diag}(\\overline y)(4\\lambda I_n - XX^T)^{-1}(\\lambda\\text{diag}(\\overline y)\\text{logit}(\\gamma) + X X^T\\text{diag}(\\overline y)\\gamma)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
